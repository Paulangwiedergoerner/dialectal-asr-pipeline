# -*- coding: utf-8 -*-
"""speech_pro_german_updated (3).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SHXF2ivZDWtJOUEsUFmMG8FeUKtiFPGB
"""

import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import tempfile

# Install required packages (Colab/Notebook environment)
!pip install -q --upgrade "datasets==2.18.0"
!pip install -q openai-whisper datasets transformers torchaudio librosa jiwer soundfile g2p_en phonemizer sentence-transformers nltk
!apt-get install -y espeak

!pip install accelerate==0.26.1 --upgrade --no-cache-dir

# Completely delete broken transformers packages and peft
!rm -rf /usr/local/lib/python3.11/dist-packages/transformers
!rm -rf /usr/local/lib/python3.11/dist-packages/transformers-*
!pip uninstall -y transformers
!pip uninstall -y peft

# Reinstall transformers and its dependencies
!pip install transformers==4.37.2 --no-cache-dir
!pip install datasets librosa jiwer soundfile wandb pyarrow git-lfs --quiet

# Explicitly install a compatible numpy version before jax
!pip install numpy>=1.21.0

!pip install jax jaxlib # Ensure jax is installed, which transformers might depend on

import os, gc, shutil, tempfile
import torch
import numpy as np
import librosa
import soundfile as sf
import matplotlib.pyplot as plt

from datasets import load_dataset, Dataset
from jiwer import wer
from difflib import SequenceMatcher
from g2p_en import G2p
from huggingface_hub import login
import nltk; nltk.download('punkt')

# Hugging Face login
login(token="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")

# Clear GPU
gc.collect()
torch.cuda.empty_cache()

# Dialect scan from Common Voice
cache_dir = "/content/hf_cache_stream"
if os.path.exists(cache_dir):
    shutil.rmtree(cache_dir)

dataset = load_dataset(
    "mozilla-foundation/common_voice_13_0",
    "de",
    split="train",
    streaming=True,
    cache_dir=cache_dir,
    trust_remote_code=True
)

dialect_map = {
    'standarddeutsch': 'Standard German',
    'hochdeutsch': 'Standard German',
    'alemannisch': 'Standard German',
    '√∂sterreichisches deutsch': 'Austrian German',
    'schweizerdeutsch': 'Switzerland German',
    'deutschland deutsch': 'Mainland German'
}

MAX_PER_DIALECT = 15
SCAN_LIMIT = 15000
dialect_counts = {label: 0 for label in set(dialect_map.values())}
dialect_samples = []

from itertools import islice
for sample in islice(dataset, SCAN_LIMIT):
    accent = (sample.get("accent") or "").lower()
    for keyword, label in dialect_map.items():
        if keyword in accent and dialect_counts[label] < MAX_PER_DIALECT:
            if sample.get("audio") and sample.get("sentence"):
                dialect_samples.append((sample, label))
                dialect_counts[label] += 1
            break
    if all(c >= MAX_PER_DIALECT for c in dialect_counts.values()):
        break

print("‚úÖ Sample counts:", dialect_counts)
native_samples = [s[0] for s in dialect_samples if s[1] in ["Mainland German", "Standard German"]]
foreign_samples = [s[0] for s in dialect_samples if s[1] not in ["Mainland German", "Standard German"]]

# ‚úÖ Import processor
from transformers import WhisperProcessor
from datasets import Dataset
import librosa
import numpy as np

# ‚úÖ Load Whisper processor
processor = WhisperProcessor.from_pretrained("openai/whisper-small", language="german", task="transcribe")

# ‚úÖ Rebuild your dataset
data = {
    "audio": [s["audio"] for s in native_samples + foreign_samples],
    "transcription": [s["sentence"] for s in native_samples + foreign_samples]
}
ds = Dataset.from_dict(data).train_test_split(test_size=0.2)

# ‚úÖ Correct preprocessing function
def prepare_dataset(batch):
    audio = batch["audio"]

    # Resample audio to 16kHz if needed
    if audio["sampling_rate"] != 16000:
        array = librosa.resample(np.array(audio["array"]), orig_sr=audio["sampling_rate"], target_sr=16000)
    else:
        array = np.array(audio["array"])

    # Extract log-Mel spectrogram features (Whisper expects [80, time])
    inputs = processor(
        array,
        sampling_rate=16000,
        return_tensors="pt"
    )

    # Whisper expects input_features of shape [batch, 80, time]
    # We store [80, time] per sample, which will be collated into batch later
    batch["input_features"] = inputs["input_features"][0]

    # Tokenize the transcription (target labels)
    batch["labels"] = processor.tokenizer(batch["transcription"]).input_ids
    return batch

# ‚úÖ Apply to dataset
ds = ds.map(prepare_dataset, remove_columns=["audio", "transcription"])

from transformers import (
    WhisperProcessor, WhisperForConditionalGeneration,
    Seq2SeqTrainingArguments, Seq2SeqTrainer
)
from sentence_transformers import SentenceTransformer, util
import wandb

# Constants
device = "cuda" if torch.cuda.is_available() else "cpu"
MAX_DURATION = 15
NUM_BEAMS = 1
NUM_RETURN = 1
VOCAB = ["ich", "du", "wir", "gehen", "kommen", "sagen", "laufen", "machen", "m√∂chten", "sehen", "wollen", "k√∂nnen"]

# Load models
processor = WhisperProcessor.from_pretrained("openai/whisper-small", language="german", task="transcribe")
model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small").to(device)
sbert = SentenceTransformer("all-MiniLM-L6-v2", device="cpu")
g2p = G2p()

from typing import List, Dict, Union
import torch
from transformers import PreTrainedTokenizerBase

class MyDataCollatorSpeechSeq2Seq:
    def __init__(self, processor, decoder_start_token_id):
        self.processor = processor
        self.decoder_start_token_id = decoder_start_token_id

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        # Separate audio inputs and labels
        # Corrected line: Pass the entire input_features tensor for padding
        input_features = [{"input_features": f["input_features"]} for f in features]
        label_features = [{"input_ids": f["labels"]} for f in features]

        batch = self.processor.feature_extractor.pad(
            input_features, return_tensors="pt"
        )
        labels_batch = self.processor.tokenizer.pad(
            label_features, return_tensors="pt"
        )
        labels = labels_batch["input_ids"].masked_fill(
            labels_batch["attention_mask"].ne(1), -100
        )

        # Optionally remove leading decoder_start_token_id if duplicated
        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().item():
            labels = labels[:, 1:]

        batch["labels"] = labels
        return batch

label

# Utility functions

def fine_tune_whisper(dataset):
    import wandb
    wandb.init(project="whisper-finetuning", name="whisper-finetuning-run", reinit=True)

    training_args = Seq2SeqTrainingArguments(
        output_dir="./whisper-finetuned",
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        learning_rate=1e-5,
        warmup_steps=50,
        max_steps=500,
        fp16=(device == "cuda"),
        evaluation_strategy="steps",
        eval_steps=50,
        logging_dir="./logs",
        save_steps=100,
        save_total_limit=2,
        report_to="wandb",
        load_best_model_at_end=True,
    )

    data_collator = MyDataCollatorSpeechSeq2Seq(
        processor=processor,
        decoder_start_token_id=model.config.decoder_start_token_id
    )

    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset["train"],
        eval_dataset=dataset["test"],
        tokenizer=processor.feature_extractor,  # required though not used
        data_collator=data_collator,
    )
    trainer.train()

def semantic_score(hyp, ref):
    h = sbert.encode(hyp, convert_to_tensor=True, normalize_embeddings=True).to(torch.float32)
    r = sbert.encode(ref, convert_to_tensor=True, normalize_embeddings=True).to(torch.float32)
    return util.pytorch_cos_sim(h, r).item()

def rerank_nbest(nbest, ref):
    return max(nbest, key=lambda hyp: semantic_score(hyp, ref))

def phoneme_similarity_de(w1, w2):
    try:
        p1 = phonemize(w1, language='de', backend='espeak', strip=True)
        p2 = phonemize(w2, language='de', backend='espeak', strip=True)
        return SequenceMatcher(None, p1, p2).ratio()
    except:
        return 0

def correct_by_phoneme_de(sentence, vocab, threshold=0.6):
    return ' '.join([
        max(vocab, key=lambda vw: phoneme_similarity_de(word, vw)) if any(phoneme_similarity_de(word, vw) >= threshold for vw in VOCAB) else word
        for word in sentence.split()
    ])

def evaluate_model(samples, max_samples=5):
    model.eval()
    references, preds_base, preds_rerank, preds_phoneme = [], [], [], []

    for i, (sample, _) in enumerate(samples[:max_samples]):
        try:
            reference = sample["sentence"].strip()
            audio = sample["audio"]
            sr = audio["sampling_rate"]
            array = np.array(audio["array"], dtype=np.float32)

            if sr != 16000:
                array = librosa.resample(array, orig_sr=sr, target_sr=16000)

            inputs = processor(array, sampling_rate=16000, return_tensors="pt")
            inputs = {k: v.to(device) for k, v in inputs.items()}

            with torch.no_grad():
                ids = model.generate(
                    inputs["input_features"],
                    attention_mask=inputs.get("attention_mask", None),
                    num_beams=5,
                    num_return_sequences=5
                )

            nbest = processor.tokenizer.batch_decode(ids, skip_special_tokens=True)
            base = nbest[0].strip()
            reranked = rerank_nbest(nbest, reference)
            phoneme_fixed = correct_by_phoneme_de(reranked, VOCAB)

            references.append(reference)
            preds_base.append(base)
            preds_rerank.append(reranked)
            preds_phoneme.append(phoneme_fixed)

            # üñ®Ô∏è Pretty print
            print(f"\nüß™ Sample #{i}")
            print("üìå Reference:         ", reference)
            print("üéØ Baseline:          ", base)
            print("üîÅ Reranked:          ", reranked)
            print("üî† Phoneme-Corrected: ", phoneme_fixed)

            del array, inputs, ids
            gc.collect()
            if device == "cuda":
                torch.cuda.empty_cache()

        except Exception as e:
            print(f"‚ùå Error on sample #{i}: {e}")
            continue

    clean_refs, clean_base, clean_rerank, clean_phoneme = [], [], [], []
    for r, b, rr, p in zip(references, preds_base, preds_rerank, preds_phoneme):
        if all([r.strip(), b.strip(), rr.strip(), p.strip()]):
            clean_refs.append(r.strip())
            clean_base.append(b.strip())
            clean_rerank.append(rr.strip())
            clean_phoneme.append(p.strip())

    print("\nüìä Evaluation Results:")
    print("Baseline WER:         ", wer(clean_refs, clean_base))
    print("Reranked WER:         ", wer(clean_refs, clean_rerank))
    print("Phoneme-Corrected WER:", wer(clean_refs, clean_phoneme))
    return clean_refs, clean_base, clean_rerank, clean_phoneme, preds_phoneme

# Fine-tune the model (takes time)
fine_tune_whisper(ds)

# Evaluate on dialect samples (first 5)
evaluate_model(dialect_samples)

# %%
# Add imports for classifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import numpy as np


feature_list = []
label_list = []

print("Extracting features for classifier training...")

device = "cuda" if torch.cuda.is_available() else "cpu"
model.eval()

with torch.no_grad():
    for sample, label in dialect_samples:
        try:
            audio = sample["audio"]
            sr = audio["sampling_rate"]
            array = np.array(audio["array"], dtype=np.float32)

            if sr != 16000:
                array = librosa.resample(array, orig_sr=sr, target_sr=16000)

            inputs = processor(array, sampling_rate=16000, return_tensors="pt")
            input_features = inputs["input_features"].to(device)

            # Get the last hidden state from the encoder
            hidden = model.model.encoder(input_features).last_hidden_state  # [1, time, 768]
            # Pool the hidden states
            pooled = torch.mean(hidden, dim=1).squeeze().cpu().numpy() # [768]

            feature_list.append(pooled)
            label_list.append(label)

        except Exception as e:
            print(f"Error extracting feature for classifier: {e}")
            continue

if not feature_list:
    print("No features extracted for classifier training. Cannot proceed.")
else:
    X = np.array(feature_list)
    y = np.array(label_list)

    # Encode labels if they are strings
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)



    # Train a classifier (e.g., RandomForestClassifier)
    print("Training classifier...")
    clf = RandomForestClassifier(n_estimators=100, random_state=42)
    clf.fit(X, y_encoded)

    # Modify transcribe_and_classify to use the label_encoder
    def transcribe_and_classify(audio_path):
        # üîä Load audio
        array, sr = librosa.load(audio_path, sr=None)
        if sr != 16000:
            array = librosa.resample(array, orig_sr=sr, target_sr=16000)

        # üéõÔ∏è Preprocess audio
        inputs = processor(array, sampling_rate=16000, return_tensors="pt")
        input_features = inputs["input_features"].to(device)
        attention_mask = inputs.get("attention_mask", None)
        if attention_mask is not None:
            attention_mask = attention_mask.to(device)

        # üéôÔ∏è Transcription
        model.eval()
        with torch.no_grad():
            ids = model.generate(
                input_features,
                attention_mask=attention_mask,
                num_beams=1, # Use 1 beam for a single transcription
                num_return_sequences=1
            )
            transcription = processor.tokenizer.batch_decode(ids, skip_special_tokens=True)[0].strip()

            # üß† Extract encoder representation (Whisper encoder) for dialect classification
            hidden = model.model.encoder(input_features).last_hidden_state  # [1, time, 768]
            pooled = torch.mean(hidden, dim=1).squeeze().cpu().numpy().reshape(1, -1) # [1, 768] for classifier

            # üîç Predict Dialect using your trained classifier
            predicted_dialect_encoded = clf.predict(pooled)[0]
            predicted_dialect_string = label_encoder.inverse_transform([predicted_dialect_encoded])[0] # Decode the prediction

            # ‚úÖ Classify as Native or Foreign based on the predicted dialect string
            if predicted_dialect_string in ["Mainland German", "Standard German"]:
                classification_label = "Native"
            else:
                classification_label = "Foreign"

        # ‚úÖ Output
        print("üìù Transcription:", transcription)
        print("üß† Predicted Dialect:", predicted_dialect_string)
        print("üß† Classification:", classification_label)
        return transcription, classification_label

    print("‚úÖ Classifier trained and ready.")
    print(f"Trained on {len(feature_list)} samples.")
    print(f"Classes: {label_encoder.classes_}")
# --- End of Classifier Training Section ---

transcribe_and_classify("/content/achtgesichterambiwasse_0008.wav")
# ‚Üí üìù Transcription: "ich gehe morgen"
# ‚Üí üß† Classification: Native