# -*- coding: utf-8 -*-
"""en_speech.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10t_foQFN1FrxNsYxQp6TOrAvE6I34k05
"""

!pip install accelerate==0.26.1 --upgrade --no-cache-dir

# Completely delete broken transformers packages and peft
!rm -rf /usr/local/lib/python3.11/dist-packages/transformers
!rm -rf /usr/local/lib/python3.11/dist-packages/transformers-*
!pip uninstall -y transformers
!pip uninstall -y peft

# Reinstall transformers and its dependencies
!pip install transformers==4.37.2 --no-cache-dir
!pip install datasets librosa jiwer soundfile wandb pyarrow git-lfs --quiet

# Explicitly install a compatible numpy version before jax
!pip install numpy>=1.21.0

!pip install jax jaxlib # Ensure jax is installed, which transformers might depend on

# Set up and clone the dataset
!git lfs install
!git clone https://huggingface.co/datasets/ylacombe/english_dialects

import pyarrow.parquet as pq
from datasets import Dataset, Audio

# Load the exact Parquet files
irish_table = pq.read_table("/content/english_dialects/irish_male/train-00000-of-00001-876ed4aebc6599d3.parquet")
scottish_table = pq.read_table([
    "/content/english_dialects/scottish_male/train-00000-of-00002-c0ace91149bc30ae.parquet",
    "/content/english_dialects/scottish_male/train-00001-of-00002-58d01ae306d0a012.parquet"
])

# Convert to Hugging Face Datasets
irish = Dataset.from_dict(irish_table.to_pydict()).cast_column("audio", Audio(sampling_rate=16000))
scottish = Dataset.from_dict(scottish_table.to_pydict()).cast_column("audio", Audio(sampling_rate=16000))

import librosa
import soundfile as sf
import tempfile
from datasets import Dataset

def prepare_dataset_for_whisper(irish, scottish, max_samples=100):
    data = {"audio": [], "transcription": []}

    for sample in irish.select(range(min(max_samples, len(irish)))):
        audio, sr = sample["audio"]["array"], sample["audio"]["sampling_rate"]
        if sr != 16000:
            audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp:
            sf.write(temp.name, audio, 16000)
            data["audio"].append(temp.name)
        data["transcription"].append(sample["text"])

    for sample in scottish.select(range(min(max_samples, len(scottish)))):
        audio, sr = sample["audio"]["array"], sample["audio"]["sampling_rate"]
        if sr != 16000:
            audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp:
            sf.write(temp.name, audio, 16000)
            data["audio"].append(temp.name)
        data["transcription"].append(sample["text"])

    dataset = Dataset.from_dict(data)
    return dataset.train_test_split(test_size=0.2, seed=42)

from transformers import WhisperProcessor, WhisperForConditionalGeneration

# Load Whisper model and processor
model_name = "openai/whisper-small"
processor = WhisperProcessor.from_pretrained(model_name)
tokenizer = processor.tokenizer  # Optional, useful for tokenizing text labels
model = WhisperForConditionalGeneration.from_pretrained(model_name)

print("‚úÖ Whisper model and processor loaded successfully.")

def prepare_dataset(dataset):
    def preprocess_function(examples):
        audio, sr = librosa.load(examples["audio"], sr=16000)
        inputs = processor.feature_extractor(audio, sampling_rate=sr, return_tensors="pt")
        labels = tokenizer(
            examples["transcription"],
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            max_length=448,
        ).input_ids
        return {
            "input_features": inputs.input_features[0],
            "labels": labels[0],
        }

    return dataset.map(preprocess_function, remove_columns=["audio", "transcription"])

from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer
from torch.nn.utils.rnn import pad_sequence
import torch
import wandb

def fine_tune_whisper(dataset):
    # Initialize W&B explicitly with project and run name
    wandb.init(project="whisper-finetuning", name="whisper-finetuning-run", reinit=True)

    training_args = Seq2SeqTrainingArguments(
        output_dir="./whisper-finetuned",
        run_name="whisper-finetuning-run",
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        learning_rate=1e-5,
        warmup_steps=50,
        max_steps=500,
        fp16=False,  # disable FP16 for CPU / non-CUDA
        evaluation_strategy="steps",
        eval_steps=50,
        logging_dir="./logs",
        logging_steps=25,
        save_steps=100,
        save_total_limit=2,
        push_to_hub=False,
        report_to="wandb",  # ‚úÖ enables logging to W&B
        load_best_model_at_end=True,
    )

    def data_collator(batch):
        input_features = [torch.tensor(ex["input_features"]) for ex in batch]
        labels = [ex["labels"] for ex in batch]
        max_len = max(f.shape[1] for f in input_features)
        padded_input_features = [torch.nn.functional.pad(f, (0, max_len - f.shape[1])) for f in input_features]
        padded_labels = pad_sequence([torch.tensor(l) for l in labels], batch_first=True, padding_value=-100)
        return {"input_features": torch.stack(padded_input_features), "labels": padded_labels}

    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset["train"],
        eval_dataset=dataset["test"],
        tokenizer=processor.feature_extractor,
        data_collator=data_collator,
    )

    trainer.train()
    trainer.save_model("./whisper-finetuned")
    processor.save_pretrained("./whisper-finetuned")
    wandb.finish()

dataset = prepare_dataset_for_whisper(irish, scottish, max_samples=100)
dataset = prepare_dataset(dataset)

print("Training...")
fine_tune_whisper(dataset)
print("Training complete!")

from jiwer import wer

def evaluate_wer(test_data):
    model = WhisperForConditionalGeneration.from_pretrained("./whisper-finetuned")
    processor = WhisperProcessor.from_pretrained("./whisper-finetuned")
    model.eval().to("cuda" if torch.cuda.is_available() else "cpu")

    references, predictions = [], []
    for sample in test_data["test"]:
        audio_path = sample["audio"]
        true = sample["transcription"]
        try:
            audio, sr = librosa.load(audio_path, sr=16000)
            inputs = processor(audio, sampling_rate=sr, return_tensors="pt").to(model.device)
            with torch.no_grad():
                pred_ids = model.generate(input_features=inputs.input_features)
            pred = processor.decode(pred_ids[0], skip_special_tokens=True)
            if pred.strip() and true.strip():
                references.append(true.strip())
                predictions.append(pred.strip())
        except Exception as e:
            print("Skipped:", e)
            continue

    # Compute WER
    score = wer(references, predictions)

    # Compute val_acc (exact match accuracy)
    correct = sum(1 for r, p in zip(references, predictions) if r.strip().lower() == p.strip().lower())
    val_acc = correct / len(references) if references else 0.0

    return score, val_acc, references, predictions

test_data = prepare_dataset_for_whisper(irish, scottish, max_samples=20)
wer_score, val_acc, refs, preds = evaluate_wer(test_data)

print(f"WER: {wer_score * 100:.2f}%")
print(f"Validation Accuracy: {val_acc * 100:.2f}%")

for r, p in zip(refs, preds):
    print(f"GT: {r}\nPR: {p}\n")

!pip install evaluate --quiet

from datasets import load_dataset, Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
import numpy as np
import evaluate

# Create dataset from your texts
texts = [sample['text'] for sample in irish] + [sample['text'] for sample in scottish]
labels = [0]*len(irish) + [1]*len(scottish)  # 0 = Irish, 1 = Scottish
dataset = Dataset.from_dict({'text': texts, 'label': labels}).train_test_split(test_size=0.2)

# Load tokenizer and model
model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

# Tokenize
def tokenize(batch):
    return tokenizer(batch["text"], padding=True, truncation=True)
dataset = dataset.map(tokenize, batched=True)

# Evaluation metric
accuracy = evaluate.load("accuracy")
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return accuracy.compute(predictions=preds, references=labels)

# Training config
args = TrainingArguments(
    output_dir="dialect_classifier",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
)

# Trainer
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

trainer.train()

trainer.save_model("dialect_classifier")
tokenizer.save_pretrained("dialect_classifier")

tokenizer = AutoTokenizer.from_pretrained("dialect_classifier")
model_classifier = AutoModelForSequenceClassification.from_pretrained("dialect_classifier")

import librosa
from transformers import WhisperProcessor, WhisperForConditionalGeneration

# Load your fine-tuned Whisper processor and model only once
processor = WhisperProcessor.from_pretrained("./whisper-finetuned")
model_whisper = WhisperForConditionalGeneration.from_pretrained("./whisper-finetuned")

# Load your trained text classifier
from transformers import AutoTokenizer, AutoModelForSequenceClassification
tokenizer = AutoTokenizer.from_pretrained("dialect_classifier")
model_classifier = AutoModelForSequenceClassification.from_pretrained("dialect_classifier")

# Predict transcription + dialect
def predict_dialect_from_audio(audio_path):
    # Transcribe using your fine-tuned Whisper model
    audio, sr = librosa.load(audio_path, sr=16000)
    inputs = processor(audio, sampling_rate=sr, return_tensors="pt")
    pred_ids = model_whisper.generate(inputs.input_features)
    transcription = processor.decode(pred_ids[0], skip_special_tokens=True)

    # Predict dialect using your trained classifier
    encoded = tokenizer(transcription, return_tensors="pt", truncation=True, padding=True)
    output = model_classifier(**encoded)
    pred = output.logits.argmax(dim=1).item()
    label = "Irish" if pred == 0 else "Scottish"

    return transcription, label

from google.colab import files

uploaded = files.upload()
filename = list(uploaded.keys())[0]

transcription, dialect = predict_dialect_from_audio(filename)

print("üó£Ô∏è Transcription:", transcription)
print("üåç Predicted Dialect:", dialect)